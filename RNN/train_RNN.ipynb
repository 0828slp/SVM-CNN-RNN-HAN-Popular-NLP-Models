{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens 80403\n",
      "Shape of Data Tensor: (12688, 200)\n",
      "Shape of Label Tensor: (12688, 2)\n",
      "Total 71291 word vectors in Glove 6B 300d.\n",
      "Bidirectional LSTM\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 200, 300)          24121200  \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 200, 300)          405900    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 300)               405900    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 302       \n",
      "=================================================================\n",
      "Total params: 24,978,452\n",
      "Trainable params: 24,978,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9196 samples, validate on 2860 samples\n",
      "Epoch 1/10\n",
      "9196/9196 [==============================] - 2883s 314ms/step - loss: 0.7058 - acc: 0.5183 - val_loss: 0.6826 - val_acc: 0.5643\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56434, saving model to model_rnn.hdf5\n",
      "- val_f1: 0.5626 - val_precision: 0.5653 - val_recall: 0.5643\n",
      "Epoch 2/10\n",
      "9196/9196 [==============================] - 1443s 157ms/step - loss: 0.6883 - acc: 0.5486 - val_loss: 0.6687 - val_acc: 0.5976\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.56434 to 0.59755, saving model to model_rnn.hdf5\n",
      "- val_f1: 0.5846 - val_precision: 0.6115 - val_recall: 0.5976\n",
      "Epoch 3/10\n",
      "9196/9196 [==============================] - 1348s 147ms/step - loss: 0.6612 - acc: 0.6013 - val_loss: 0.6533 - val_acc: 0.6108\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.59755 to 0.61084, saving model to model_rnn.hdf5\n",
      "- val_f1: 0.6065 - val_precision: 0.6159 - val_recall: 0.6108\n",
      "Epoch 4/10\n",
      "9196/9196 [==============================] - 1328s 144ms/step - loss: 0.5926 - acc: 0.6828 - val_loss: 0.7472 - val_acc: 0.5769\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61084\n",
      "- val_f1: 0.5290 - val_precision: 0.6297 - val_recall: 0.5769\n",
      "Epoch 5/10\n",
      "9196/9196 [==============================] - 1285s 140ms/step - loss: 0.4869 - acc: 0.7611 - val_loss: 0.7521 - val_acc: 0.6171\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.61084 to 0.61713, saving model to model_rnn.hdf5\n",
      "- val_f1: 0.6136 - val_precision: 0.6216 - val_recall: 0.6171\n",
      "Epoch 6/10\n",
      "9196/9196 [==============================] - 1357s 148ms/step - loss: 0.3412 - acc: 0.8466 - val_loss: 0.9083 - val_acc: 0.6070\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.61713\n",
      "- val_f1: 0.6064 - val_precision: 0.6077 - val_recall: 0.6070\n",
      "Epoch 7/10\n",
      "4976/9196 [===============>..............] - ETA: 9:16 - loss: 0.1921 - acc: 0.9208"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint,Callback\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,accuracy_score\n",
    "# from sklearn import metrics \n",
    "# %matplotlib inline\n",
    "\n",
    "def Get_Accuracy(y_true, y_pred): #Accuracy 准确率：分类器正确分类的样本数与总样本数之比 \n",
    "#    accuracy = accuracy_score(y_true,y_pred,normalize = False) \n",
    "    accuracy = accuracy_score(y_true,y_pred)\n",
    "    return accuracy\n",
    "\n",
    "def Get_Precision_score(y_true, y_pred): #Precision：精准率 正确被预测的正样本(TP)占所有被预测为正样本(TP+FP)的比例. \n",
    "    precision = precision_score(y_true,y_pred,average='weighted')  \n",
    "    return precision\n",
    "\n",
    "def Get_Recall(y_true, y_pred): #Recall 召回率 正确被预测的正样本(TP)占所有真正 正样本(TP+FN)的比例.  \n",
    "    Recall = recall_score(y_true,y_pred,average='weighted')  \n",
    "    return Recall \n",
    " \n",
    "def Get_f1_score(y_true, y_pred): #F1-score: 精确率(precision)和召回率(Recall)的调和平均数  \n",
    "    f1_score1 = f1_score(y_true,y_pred,average='weighted')  \n",
    "    return f1_score1\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def __init__(self):\n",
    "        self.predict = []\n",
    "        self.target = []\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        self.confusion_matrixs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict,average='weighted')\n",
    "        _val_recall = recall_score(val_targ, val_predict,average='weighted')\n",
    "        _val_precision = precision_score(val_targ, val_predict,average='weighted')\n",
    "#        self.confusion_matrixs = confusion_matrix(val_targ,val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print('- val_f1: %.4f - val_precision: %.4f - val_recall: %.4f'%(_val_f1, _val_precision, _val_recall))\n",
    "        return\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2\n",
    "DROP_OUT_LAYER = 0.2\n",
    "\n",
    "# reading data\n",
    "df1 = pd.read_excel('train.xls')\n",
    "df1 = df1.dropna()\n",
    "df1 = df1.reset_index(drop=True)\n",
    "SPLIT_LINE = df1.Deal_editorial.shape[0]\n",
    "df2 = pd.read_excel('valid.xls')\n",
    "df2 = df2.dropna()\n",
    "df2 = df2.reset_index(drop=True)\n",
    "TEST_LINE = df2.Deal_editorial.shape[0]\n",
    "df3 = pd.read_excel('test.xls')\n",
    "df3 = df3.dropna()\n",
    "df3 = df3.reset_index(drop=True)\n",
    "df = df1.append(df2).append(df3)\n",
    "# print('Shape of dataset ',df.shape)\n",
    "# print(df.columns)\n",
    "\n",
    "macronum=sorted(set(df['Deal_status']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "df['Deal_status']=df['Deal_status'].apply(fun)\n",
    "\n",
    "texts = []\n",
    "for i in range(len(list(df['Deal_editorial']))):\n",
    "    texts.append(list(df['Deal_editorial'])[i].replace(\"\\n\",\"\"))\n",
    "labels = []\n",
    "\n",
    "# for idx in range(df.Deal_editorial.shape[0]):\n",
    "#     text = BeautifulSoup(df.Deal_editorial[idx])\n",
    "#     texts.append(clean_str(str(text.get_text().encode())))\n",
    "\n",
    "for idx in df['Deal_status']:\n",
    "    labels.append(idx)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens',len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of Data Tensor:', data.shape)\n",
    "print('Shape of Label Tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:SPLIT_LINE]\n",
    "y_train = labels[:SPLIT_LINE]\n",
    "x_val = data[SPLIT_LINE:SPLIT_LINE+TEST_LINE]\n",
    "y_val = labels[SPLIT_LINE:SPLIT_LINE+TEST_LINE]\n",
    "x_test = data[SPLIT_LINE+TEST_LINE:]\n",
    "y_test = labels[SPLIT_LINE+TEST_LINE:]\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('../GloVe/vectors.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 300d.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "# sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sequence_input)\n",
    "# l_lstm = Bidirectional(LSTM(150))(embedded_sequences)\n",
    "# l_dropout = Dropout(DROP_OUT_LAYER)(l_lstm)\n",
    "# preds = Dense(len(macronum), activation='softmax')(l_dropout)\n",
    "# model = Model(sequence_input, preds)\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "lstm_word1 = Bidirectional(GRU(150, return_sequences=True))(embedded_sequences)\n",
    "l_dropout1 = Dropout(0.2)(lstm_word1)\n",
    "lstm_word2 = Bidirectional(GRU(150, return_sequences=False))(l_dropout1)\n",
    "l_dropout2 = Dropout(0.2)(lstm_word2)\n",
    "l_dense = Dense(150, activation='relu')(l_dropout2)\n",
    "l_dropout3 = Dropout(0.2)(l_dense)\n",
    "preds = Dense(2, activation='softmax')(l_dropout3)\n",
    "\n",
    "# attn_word = HierarchicalAttentionNetwork(150)(lstm_word)\n",
    "# model = Model(sentence_input, attn_word)\n",
    "\n",
    "# review_input = Input(shape=(MAX_SENT_LENGTH, MAX_SENTS), dtype='int32')\n",
    "# review_encoder = TimeDistributed(sentenceEncoder)(review_input)\n",
    "# lstm_sentence = Bidirectional(GRU(150, return_sequences=True))(review_encoder)\n",
    "# attn_sentence = HierarchicalAttentionNetwork(150)(lstm_sentence)\n",
    "# preds = Dense(len(macronum), activation='softmax')(attn_sentence)\n",
    "model = Model(sentence_input, preds)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Bidirectional LSTM\")\n",
    "model.summary()\n",
    "\n",
    "cp=ModelCheckpoint('model_rnn.hdf5',monitor='val_acc',verbose=1,save_best_only=True)\n",
    "metrics = Metrics()\n",
    "history=model.fit(x_train, y_train, validation_data=(x_val, y_val),epochs=10, batch_size=8,callbacks=[cp,metrics])\n",
    "\n",
    "print(history)\n",
    "#print(\"confusion--->>\",metrics.confusion_matrixs)\n",
    "print(\"f1_score-->>\",metrics.val_f1s)\n",
    "print(\"precision---->>\",metrics.val_precisions)\n",
    "print(\"recalls----->>\",metrics.val_recalls)\n",
    "\n",
    "y_predict1 = model.predict(x_test)\n",
    "\n",
    "y_predict = (y_predict1>0.5)\n",
    "accuracy = Get_Accuracy(y_test,y_predict)\n",
    "print(\"RNN Accuracy_Score = %f\"%accuracy) \n",
    "precision = Get_Precision_score(y_test,y_predict)\n",
    "print(\"RNN Precision = %f\"%precision)\n",
    "recall = Get_Recall(y_test,y_predict)\n",
    "print(\"RNN Recall = %f\"%recall) \n",
    "f1_score1 = Get_f1_score(y_test,y_predict)\n",
    "print(\"RNN F1-Score  = %f\"%f1_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.620253164556962"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
